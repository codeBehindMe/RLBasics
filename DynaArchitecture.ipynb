{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dyna Architecture for Model Based RL\n",
    "\n",
    "* In direct RL the agent learns from experience genrated by interacting with the world like in Q-Learning.\n",
    "* The agent can also generate simulated experience from a model and use it for planning.\n",
    "\n",
    "* Q-planning uses simulated model experience to update the value functions. \n",
    "\n",
    "## Dyna\n",
    "The dyna architecture looks as follows.\n",
    "![](images/dyna_architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the above, we have the usual environment - policy interaction ,  which is the real experience, which we use to do direct RL updates.\n",
    "* Then we use that real experience to also updated the model.\n",
    "* In addition, we want to control how the model generates simulated experience which the agent plans from, this is called search control.\n",
    "* Planning updates are then performed using the simulated experience.\n",
    "\n",
    "* Dyna unifies planning, learning and acting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Dyna-Q\n",
    "\n",
    "* Tabular Dyna-Q assumes deterministic transitions\n",
    "\n",
    "The Tabular Dyna-Q Algorithm\n",
    "```\n",
    "01. Initialise Q(s,a) and Model(s,a) for all s in S and a in A(s)\n",
    "\n",
    "02. Loop forever:\n",
    "03.   (a) S <- current(nonterminal) state\n",
    "04.   (b) A <- epsilon-greedy(S,Q) \n",
    "05.   (c) Take action A; observe reward R and next state S'\n",
    "06.   (d) Q(S,A) <- Q(S,A) + alpha [R + gamma * max_a{Q(S',a)} - Q(S,A)]\n",
    "07.   (e) Model(S,A) <- R, S' (assuming deterministic environment)\n",
    "\n",
    "# Start planning\n",
    "         # Selectl a previously visited state action pair at random\n",
    "08.      S <- random previously observed state\n",
    "09.      A <- random action previously taken in S\n",
    "         # Query the model for model transition to generate R and S'\n",
    "10.      R, S' <- Model(S,A)\n",
    "         # Perform a Q-learning update with the simulated reward and next state.\n",
    "11.      Q(S,A) <- Q(S,A) + alpha [R + gamma * max_a{Q(S',a)} - Q(S,A)]\n",
    "```\n",
    "\n",
    "* The planning step is performed many times.\n",
    "* Dyna-Q performs many planning updates **for each environment transition.**\n",
    "* Dyna-Q makes better use of its limited interraction with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with innacurate models\n",
    "\n",
    "Models can be inaccurate in two ways.\n",
    "* Firstly, the model may not have seen many of the states or taken many of the actions in these states. This is called an **incomplete model**.\n",
    "* The model can also be inaccurate if the environment changes, so taking a particular action in a state may result in a different next state and reward to what hte model observed; we say that this model is **inaccurate** in this case.\n",
    "\n",
    "* Changes in the environment leads to innacurate models.\n",
    "\n",
    "### Planning with an innacurate model.\n",
    "\n",
    "* If the environment changes and the model no-longer reflects the transitions that might happen in the envinronemnt, planning using this model may lead to updating the value function in the wrong direction.\n",
    "\n",
    "* Dyna-Q plans with an incomplete model by only sampling state-action pairs that have been previously visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
